#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Stage-1 training (single-file):
- Freeze tokenizer/VAE
- Train Gate (entropy_spatial teacher) + LoRA finetune Infinity VAR
- Offline FLAN-T5-XL encoder embeddings (no flan-t5-xl during training)
- Random k_transmit per step
- Random keep_ratio per scale per step
- FSDP supported (0=DDP, 2=SHARD_GRAD_OP, 3=FULL_SHARD)
- Periodic validation with pyiqa (LPIPS + DISTS), save best
- Save ONLY trainable params (Gate + LoRA trainables)

Run:
torchrun --nproc_per_node=8 train_stage1_gate_lora_fsdp.py \
  --train_json /path/train.json --val_json /path/val.json \
  --emb_dir /path/emb_out \
  --vae_ckpt /path/vae.ckpt --model_ckpt /path/infinity_pretrained.pt \
  --out_dir ./stage1_out \
  --fixed_hw 512,1024 \
  --batch_size 2 --epochs 1 \
  --k_transmit_min 2 --k_transmit_max 6 \
  --keep_ratio_min 0.05 --keep_ratio_max 0.35 \
  --lora_r 8 --lora_alpha 16 \
  --fsdp 3 --val_every 500 --val_num 16

Note:
- emb_dir should be generated by your precompute script (index.jsonl + shards/shard_*.pt).
"""

import argparse
import json
import math
import os
import random
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.utils.data import Dataset, DataLoader, DistributedSampler

# -------------------------
# FSDP imports
# -------------------------
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import ShardingStrategy
from torch.distributed.fsdp.api import FullStateDictConfig, StateDictType
from torch.distributed.fsdp.wrap import ModuleWrapPolicy

# -------------------------
# Try import your Infinity stack with fallbacks
# -------------------------
def _import_infinity():
    # VAE
    try:
        from infinity.models.bsq_vae.vae import vae_model  # your arpc_cli uses this
    except Exception:
        from infinity.models.bsq_vae.modeling_bsq_vae import vae_model  # fallback
    # schedule
    try:
        from infinity.models.bsq_vae.multiscale_bsq import get_latent2scale_schedule
    except Exception:
        # sometimes schedule is under utils
        from infinity.utils.dynamic_resolution import dynamic_resolution_h_w as get_latent2scale_schedule  # last resort (may not match)
    # InfinityPatched + wrapper chunk module
    try:
        from infinity.models.infinity_patched import Infinity as InfinityPatched
        from infinity.models.infinity_patched import MultipleLayers
    except Exception:
        from infinity_patched import Infinity as InfinityPatched  # local fallback
        from infinity_patched import MultipleLayers
    # BitwiseSelfCorrection (you uploaded it as a standalone file in your repo)
    try:
        from bitwise_self_correction import BitwiseSelfCorrection
    except Exception:
        from infinity.models.bitwise_self_correction import BitwiseSelfCorrection  # fallback

    return vae_model, get_latent2scale_schedule, InfinityPatched, MultipleLayers, BitwiseSelfCorrection


vae_model, get_latent2scale_schedule, InfinityPatched, MultipleLayers, BitwiseSelfCorrection = _import_infinity()


# =========================================================
# Offline T5 embedding store (reads emb_dir/index.jsonl + shards)
# =========================================================
class OfflineT5EmbeddingStore:
    """
    emb_dir/
      index.jsonl   # {"path": ..., "shard": int, "idx": int}
      shards/shard_000000.pt:
        paths: [N]
        lens: [N] int32
        offsets: [N+1] int64
        kv_compact: [sum_len, Ct5] fp16
    """
    def __init__(self, emb_dir: str):
        self.emb_dir = Path(emb_dir)
        self.index_path = self.emb_dir / "index.jsonl"
        self.shards_dir = self.emb_dir / "shards"
        if not self.index_path.exists():
            raise FileNotFoundError(f"Missing {self.index_path}")
        if not self.shards_dir.exists():
            raise FileNotFoundError(f"Missing {self.shards_dir}")

        self._index: Dict[str, Tuple[int, int]] = {}
        with open(self.index_path, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                rec = json.loads(line)
                self._index[rec["path"]] = (int(rec["shard"]), int(rec["idx"]))

        self._shard_cache: Dict[int, Dict[str, Any]] = {}

    def _load_shard(self, shard_id: int) -> Dict[str, Any]:
        if shard_id in self._shard_cache:
            return self._shard_cache[shard_id]
        shard_path = self.shards_dir / f"shard_{shard_id:06d}.pt"
        pack = torch.load(shard_path, map_location="cpu")
        self._shard_cache[shard_id] = pack
        return pack

    def get(self, img_path: str) -> Tuple[torch.Tensor, int]:
        if img_path not in self._index:
            raise KeyError(f"Image path not found in embedding index: {img_path}")
        shard_id, idx = self._index[img_path]
        pack = self._load_shard(shard_id)
        offsets = pack["offsets"]
        lens = pack["lens"]
        kv_compact = pack["kv_compact"]
        st = int(offsets[idx].item())
        ed = int(offsets[idx + 1].item())
        le = int(lens[idx].item())
        kv = kv_compact[st:ed]  # [le, Ct5], fp16 CPU
        if kv.shape[0] != le:
            raise RuntimeError(f"KV length mismatch: {kv.shape[0]} vs {le} for {img_path}")
        return kv, le


def collate_text_cond_tuple(batch_kv_lens: List[Tuple[torch.Tensor, int]], device: torch.device):
    """
    Build Infinity text_cond_tuple:
      kv_compact: [sum_len, Ct5] fp16
      lens: [B] int32
      cu_seqlens_k: [B+1] int32 (prefix sum)
      max_seqlen_k: int
    """
    lens = torch.tensor([le for (_, le) in batch_kv_lens], dtype=torch.int32, device=device)
    max_seqlen_k = int(lens.max().item()) if lens.numel() else 0
    cu = torch.zeros((lens.shape[0] + 1,), device=device, dtype=torch.int32)
    cu[1:] = torch.cumsum(lens, dim=0)
    kv = torch.cat([kv.to(device=device, dtype=torch.float16) for (kv, _) in batch_kv_lens], dim=0).contiguous()
    return kv, lens, cu, max_seqlen_k


# =========================================================
# Minimal image preprocessing (no torchvision)
# =========================================================
def _pil_resize_shorter_side(im: Image.Image, target: int) -> Image.Image:
    w, h = im.size
    if w == 0 or h == 0:
        raise ValueError("Invalid image size")
    scale = target / min(w, h)
    nw, nh = int(round(w * scale)), int(round(h * scale))
    return im.resize((nw, nh), resample=Image.BICUBIC)

def _pil_center_crop(im: Image.Image, size: int) -> Image.Image:
    w, h = im.size
    left = max(0, (w - size) // 2)
    top = max(0, (h - size) // 2)
    return im.crop((left, top, left + size, top + size))

def load_image_to_tensor(img_path: str, size: int) -> torch.Tensor:
    """
    Returns tensor [3,size,size] normalized to [-1,1].
    """
    with Image.open(img_path) as im:
        im = im.convert("RGB")
        im = _pil_resize_shorter_side(im, size)
        im = _pil_center_crop(im, size)
        arr = np.asarray(im).astype(np.float32) / 255.0  # [H,W,3]
        ten = torch.from_numpy(arr).permute(2, 0, 1)  # [3,H,W]
        ten = ten * 2.0 - 1.0
        return ten


# =========================================================
# Dataset
# =========================================================
class FlickrJsonOfflineT5(Dataset):
    """
    JSON: { "/abs/path/img.jpg": ["cap1", ...], ... }
    Uses img path as key to read offline embeddings.
    """
    def __init__(self, json_path: str, emb_dir: str, res_list: List[int], seed: int = 0):
        super().__init__()
        self.json_path = str(json_path)
        self.res_list = list(res_list)
        if not self.res_list:
            raise ValueError("res_list is empty")
        self.rng = random.Random(seed)
        self.emb = OfflineT5EmbeddingStore(emb_dir)

        with open(self.json_path, "r") as f:
            j = json.load(f)

        self.img_paths = list(j.keys())

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx: int):
        img_path = self.img_paths[idx]
        res = self.res_list[0] if len(self.res_list) == 1 else self.rng.choice(self.res_list)

        x = load_image_to_tensor(img_path, res)  # [3,res,res], [-1,1]
        kv, le = self.emb.get(img_path)
        return {"img": x, "kv": kv, "len": le, "path": img_path, "res": res}


# =========================================================
# LoRA for nn.Linear (single-file)
# =========================================================
class LoRALinear(nn.Module):
    def __init__(self, base: nn.Linear, r: int, alpha: float, dropout: float = 0.0):
        super().__init__()
        if not isinstance(base, nn.Linear):
            raise TypeError("LoRALinear expects nn.Linear")
        self.base = base
        self.in_features = base.in_features
        self.out_features = base.out_features
        self.r = int(r)
        self.alpha = float(alpha)
        self.scaling = self.alpha / max(1, self.r)
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

        for p in self.base.parameters():
            p.requires_grad = False

        if self.r > 0:
            self.lora_A = nn.Parameter(torch.zeros((self.r, self.in_features), dtype=torch.float32))
            self.lora_B = nn.Parameter(torch.zeros((self.out_features, self.r), dtype=torch.float32))
            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
            nn.init.zeros_(self.lora_B)
        else:
            self.register_parameter("lora_A", None)
            self.register_parameter("lora_B", None)

    def forward(self, x):
        y = self.base(x)
        if self.r <= 0:
            return y
        x_d = self.dropout(x)
        l = F.linear(F.linear(x_d, self.lora_A), self.lora_B) * self.scaling
        return y + l


def _get_parent_module(root: nn.Module, name: str) -> Tuple[nn.Module, str]:
    parts = name.split(".")
    parent = root
    for p in parts[:-1]:
        parent = getattr(parent, p)
    return parent, parts[-1]


def inject_lora(
    model: nn.Module,
    r: int,
    alpha: float,
    dropout: float,
    target_regex: str,
    exclude_regex: Optional[str] = None,
    verbose: bool = True,
) -> nn.Module:
    import re
    tgt = re.compile(target_regex)
    ex = re.compile(exclude_regex) if exclude_regex else None

    to_replace: List[str] = []
    for n, m in model.named_modules():
        if isinstance(m, nn.Linear):
            if not tgt.search(n):
                continue
            if ex and ex.search(n):
                continue
            to_replace.append(n)

    for n in to_replace:
        parent, leaf = _get_parent_module(model, n)
        base = getattr(parent, leaf)
        setattr(parent, leaf, LoRALinear(base=base, r=r, alpha=alpha, dropout=dropout))
        if verbose:
            print(f"[LoRA] replaced Linear: {n}")

    return model


# =========================================================
# Gate network (single-file)
# =========================================================
class GateNet(nn.Module):
    def __init__(self, in_ch: int, hidden: int = 128, depth: int = 3):
        super().__init__()
        layers: List[nn.Module] = []
        ch = in_ch
        depth = max(1, int(depth))
        for _ in range(depth):
            layers.append(nn.Conv2d(ch, hidden, kernel_size=3, padding=1))
            layers.append(nn.SiLU(inplace=True))
            ch = hidden
        self.backbone = nn.Sequential(*layers)
        self.head = nn.Conv2d(ch, 1, kernel_size=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: [B,C,H,W] -> [B,1,H,W]
        return self.head(self.backbone(x))


def topk_mask_from_scores(scores_BHW: torch.Tensor, keep_ratio: float) -> torch.Tensor:
    """
    scores_BHW: [B,H,W], larger => keep
    keep_ratio: (0,1]
    returns bool mask [B,H,W] with exactly topk per sample.
    """
    B, H, W = scores_BHW.shape
    L = H * W
    k = max(1, min(L, int(round(L * float(keep_ratio)))))
    flat = scores_BHW.reshape(B, L)
    _, idx = torch.topk(flat, k=k, dim=1, largest=True, sorted=False)
    mask = torch.zeros((B, L), device=scores_BHW.device, dtype=torch.bool)
    mask.scatter_(1, idx, True)
    return mask.reshape(B, H, W)


# =========================================================
# Distributed helpers
# =========================================================
def ddp_setup(seed: int):
    if dist.is_initialized():
        return
    dist.init_process_group(backend="nccl")
    torch.cuda.set_device(int(os.environ.get("LOCAL_RANK", "0")))
    rank = dist.get_rank()
    torch.manual_seed(seed + rank)
    np.random.seed(seed + rank)
    random.seed(seed + rank)


def is_main() -> bool:
    return (not dist.is_initialized()) or dist.get_rank() == 0


# =========================================================
# Core training utils
# =========================================================
def bits_loss_from_logits(logits_BLV: torch.Tensor, gt_BLd: torch.Tensor) -> torch.Tensor:
    """
    logits_BLV: [B,L,2*d]
    gt_BLd: [B,L,d] in {0,1} long
    """
    B, L, V = logits_BLV.shape
    d = gt_BLd.shape[-1]
    if V != 2 * d:
        raise RuntimeError(f"logits last dim V={V} must equal 2*d={2*d}")
    logits = logits_BLV.reshape(B, L, d, 2).permute(0, 3, 1, 2)  # [B,2,L,d]
    return F.cross_entropy(logits, gt_BLd, reduction="mean")


def p1_from_logits(logits_BLV: torch.Tensor) -> torch.Tensor:
    """Return p(bit=1): [B,L,d]."""
    B, L, V = logits_BLV.shape
    d = V // 2
    logits2 = logits_BLV.reshape(B, L, d, 2)
    return torch.softmax(logits2, dim=-1)[..., 1]


def binary_entropy(p: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
    p = p.clamp(eps, 1 - eps)
    return -(p * torch.log2(p) + (1 - p) * torch.log2(1 - p))


def segment_x_tokens_to_scale(x_BLC_wo_prefix: torch.Tensor, scale_schedule: List[Tuple[int, int, int]], scale_idx: int) -> torch.Tensor:
    """
    x_BLC_wo_prefix is concat of inputs for scale 1..K-1.
    scale_schedule is list of (pt, ph, pw) for scales 0..K-1.
    For scale_idx>=1, returns [B,C,H,W] feature map for that scale.
    """
    assert scale_idx >= 1
    lens_each = [int(np.prod(s)) for s in scale_schedule]  # per-scale token count
    start = int(sum(lens_each[1:scale_idx]))
    end = start + lens_each[scale_idx]
    seg = x_BLC_wo_prefix[:, start:end, :]  # [B, Ls, C]
    _, ph, pw = scale_schedule[scale_idx]
    return seg.permute(0, 2, 1).reshape(seg.shape[0], seg.shape[2], ph, pw)


def teacher_entropy_spatial_mask(
    logits_BLV: torch.Tensor,
    scale_schedule: List[Tuple[int, int, int]],
    scale_idx: int,
    d_eff: int,
    keep_ratio: float,
) -> torch.Tensor:
    """
    Teacher mask: topk spatial positions by mean bit-entropy for that scale.
    returns bool [B,H,W]
    """
    p1 = p1_from_logits(logits_BLV)  # [B,L,d]
    lens_each = [int(np.prod(s)) for s in scale_schedule]
    start = int(sum(lens_each[:scale_idx]))
    end = start + lens_each[scale_idx]
    p_scale = p1[:, start:end, :d_eff]  # [B,Ls,d_eff]
    Hpos = binary_entropy(p_scale).mean(dim=-1)  # [B,Ls]
    _, ph, pw = scale_schedule[scale_idx]
    return topk_mask_from_scores(Hpos.reshape(Hpos.shape[0], ph, pw), keep_ratio=keep_ratio)


def trainable_state_dict_fsdp(model: nn.Module) -> Dict[str, torch.Tensor]:
    """
    Save ONLY requires_grad params.
    Works for FSDP(use_orig_params=True) by taking FULL_STATE_DICT on rank0.
    """
    trainable_names = {n for (n, p) in model.named_parameters() if p.requires_grad}
    if isinstance(model, FSDP):
        cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
        with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, cfg):
            sd = model.state_dict()
        if not is_main():
            return {}
        return {k: v for k, v in sd.items() if k in trainable_names}
    else:
        return {n: p.detach().cpu() for (n, p) in model.named_parameters() if p.requires_grad}


# =========================================================
# Args
# =========================================================
def parse_args():
    p = argparse.ArgumentParser()

    # data
    p.add_argument("--train_json", type=str, required=True)
    p.add_argument("--val_json", type=str, required=True)
    p.add_argument("--emb_dir", type=str, required=True)
    p.add_argument("--fixed_hw", type=str, default="512", help="e.g. '512' or '512,1024' for random choice")
    p.add_argument("--num_workers", type=int, default=8)

    # model
    p.add_argument("--text_channels", type=int, default=2048)
    p.add_argument("--tlen", type=int, default=512)
    p.add_argument("--vae_ckpt", type=str, required=True)
    p.add_argument("--vae_type", type=int, default=32)
    p.add_argument("--apply_spatial_patchify", type=int, default=0, choices=[0, 1])
    p.add_argument("--model_ckpt", type=str, required=True)
    p.add_argument("--model_type", type=str, default="infinity_2b")
    p.add_argument("--use_bit_label", type=int, default=1, choices=[0, 1])
    p.add_argument("--add_lvl_embeding_only_first_block", type=int, default=1, choices=[0, 1])
    p.add_argument("--rope2d_each_sa_layer", type=int, default=0, choices=[0, 1])
    p.add_argument("--rope2d_normalized_by_hw", type=int, default=0, choices=[0, 1, 2])
    p.add_argument("--use_flex_attn", type=int, default=0, choices=[0, 1])
    p.add_argument("--always_training_scales", type=int, default=20)

    # training
    p.add_argument("--out_dir", type=str, required=True)
    p.add_argument("--epochs", type=int, default=1)
    p.add_argument("--batch_size", type=int, default=2)
    p.add_argument("--lr", type=float, default=1e-4)
    p.add_argument("--gate_lr", type=float, default=2e-4)
    p.add_argument("--weight_decay", type=float, default=0.0)
    p.add_argument("--grad_clip", type=float, default=1.0)
    p.add_argument("--bf16", type=int, default=1, choices=[0, 1])
    p.add_argument("--seed", type=int, default=0)

    # random budgets
    p.add_argument("--k_transmit_min", type=int, default=2)
    p.add_argument("--k_transmit_max", type=int, default=6)
    p.add_argument("--keep_ratio_min", type=float, default=0.05)
    p.add_argument("--keep_ratio_max", type=float, default=0.35)

    # LoRA
    p.add_argument("--lora_r", type=int, default=8)
    p.add_argument("--lora_alpha", type=float, default=16.0)
    p.add_argument("--lora_dropout", type=float, default=0.0)
    p.add_argument("--lora_target_regex", type=str, default=r"(attn|mat_|fc|proj|qkv|mlp)")
    p.add_argument("--lora_exclude_regex", type=str, default=r"(head|word_embed|text_)")

    # GateNet
    p.add_argument("--gate_hidden", type=int, default=128)
    p.add_argument("--gate_depth", type=int, default=3)
    p.add_argument("--gate_loss_weight", type=float, default=1.0)

    # validation
    p.add_argument("--val_every", type=int, default=500)
    p.add_argument("--val_num", type=int, default=16)
    p.add_argument("--val_keep_ratio", type=float, default=0.25)
    p.add_argument("--val_k_transmit", type=int, default=5)

    # fsdp
    p.add_argument("--fsdp", type=int, default=0, choices=[0, 2, 3], help="0=DDP, 2=SHARD_GRAD_OP, 3=FULL_SHARD")

    return p.parse_args()


# =========================================================
# Model type mapping (match your CLI style)
# =========================================================
def model_kwargs_from_type(model_type: str) -> Dict[str, Any]:
    if model_type == "infinity_2b":
        return dict(depth=32, embed_dim=2048, num_heads=2048 // 128, drop_path_rate=0.1, mlp_ratio=4, block_chunks=8)
    if model_type == "infinity_8b":
        return dict(depth=40, embed_dim=3584, num_heads=28, drop_path_rate=0.1, mlp_ratio=4, block_chunks=8)
    if model_type == "infinity_layer12":
        return dict(depth=12, embed_dim=768, num_heads=8, drop_path_rate=0.1, mlp_ratio=4, block_chunks=4)
    if model_type == "infinity_layer16":
        return dict(depth=16, embed_dim=1152, num_heads=12, drop_path_rate=0.1, mlp_ratio=4, block_chunks=4)
    if model_type == "infinity_layer24":
        return dict(depth=24, embed_dim=1536, num_heads=16, drop_path_rate=0.1, mlp_ratio=4, block_chunks=4)
    if model_type == "infinity_layer32":
        return dict(depth=32, embed_dim=2080, num_heads=20, drop_path_rate=0.1, mlp_ratio=4, block_chunks=4)
    if model_type == "infinity_layer40":
        return dict(depth=40, embed_dim=2688, num_heads=24, drop_path_rate=0.1, mlp_ratio=4, block_chunks=4)
    if model_type == "infinity_layer48":
        return dict(depth=48, embed_dim=3360, num_heads=28, drop_path_rate=0.1, mlp_ratio=4, block_chunks=4)
    raise ValueError(f"unknown model_type: {model_type}")


# =========================================================
# Build VAE / GPT
# =========================================================
@torch.no_grad()
def build_vae(args, device: torch.device):
    schedule_mode = "dynamic"
    codebook_dim = int(args.vae_type)
    codebook_size = 2 ** codebook_dim

    if args.apply_spatial_patchify:
        patch_size = 8
        encoder_ch_mult = [1, 2, 4, 4]
        decoder_ch_mult = [1, 2, 4, 4]
    else:
        patch_size = 16
        encoder_ch_mult = [1, 2, 4, 4, 4]
        decoder_ch_mult = [1, 2, 4, 4, 4]

    vae = vae_model(
        vqgan_ckpt=args.vae_ckpt,
        schedule_mode=schedule_mode,
        codebook_dim=codebook_dim,
        codebook_size=codebook_size,
        test_mode=True,
        patch_size=patch_size,
        encoder_ch_mult=encoder_ch_mult,
        decoder_ch_mult=decoder_ch_mult,
    ).to(device)
    vae.eval()
    vae.requires_grad_(False)
    return vae


@torch.no_grad()
def build_gpt(args, vae, device: torch.device):
    gpt = InfinityPatched(
        vae_local=vae,
        text_channels=int(args.text_channels),
        text_maxlen=int(args.tlen),
        shared_aln=True,
        raw_scale_schedule=None,
        checkpointing="full-block",
        customized_flash_attn=False,
        fused_norm=True,
        pad_to_multiplier=128,
        use_flex_attn=bool(args.use_flex_attn),
        add_lvl_embeding_only_first_block=int(args.add_lvl_embeding_only_first_block),
        use_bit_label=int(args.use_bit_label),
        rope2d_each_sa_layer=int(args.rope2d_each_sa_layer),
        rope2d_normalized_by_hw=int(args.rope2d_normalized_by_hw),
        pn="1M",
        apply_spatial_patchify=int(args.apply_spatial_patchify),
        inference_mode=False,
        train_h_div_w_list=[1.0],
        **model_kwargs_from_type(args.model_type),
    ).to(device)

    ckpt = torch.load(args.model_ckpt, map_location="cpu")
    state = ckpt
    if isinstance(ckpt, dict):
        for key in ["state_dict", "model", "gpt", "ema"]:
            if key in ckpt and isinstance(ckpt[key], dict):
                state = ckpt[key]
                break
    missing, unexpected = gpt.load_state_dict(state, strict=False)
    if is_main():
        print(f"[LOAD GPT] missing={len(missing)} unexpected={len(unexpected)}")
    return gpt


def wrap_fsdp_if_needed(model: nn.Module, args) -> nn.Module:
    if args.fsdp == 0:
        return model

    if args.fsdp == 3:
        sharding = ShardingStrategy.FULL_SHARD
    else:
        sharding = ShardingStrategy.SHARD_GRAD_OP

    # auto-wrap: prefer MultipleLayers (chunk wrapper), else fallback to first block type
    wrap_types = [MultipleLayers]
    if hasattr(model, "unregistered_blocks") and isinstance(model.unregistered_blocks, list) and len(model.unregistered_blocks) > 0:
        wrap_types.append(type(model.unregistered_blocks[0]))
    auto_wrap_policy = ModuleWrapPolicy(wrap_types)

    return FSDP(
        model,
        device_id=torch.cuda.current_device(),
        sharding_strategy=sharding,
        auto_wrap_policy=auto_wrap_policy,
        use_orig_params=True,
        sync_module_states=True,
        limit_all_gathers=True,
    )


# =========================================================
# Validation (pyiqa)
# =========================================================
@torch.no_grad()
def validate(args, vae, gpt, gate, ld_val, device: torch.device) -> Tuple[float, Dict[str, float]]:
    import pyiqa

    lpips = pyiqa.create_metric("lpips", device=device)
    dists = pyiqa.create_metric("dists", device=device)

    gpt.eval()
    gate.eval()

    k_transmit = int(args.val_k_transmit)
    keep_ratio = float(args.val_keep_ratio)

    tot = 0
    sum_lp, sum_di = 0.0, 0.0

    for img_B3HW, kv_lens in ld_val:
        if tot >= int(args.val_num):
            break
        img_B3HW = img_B3HW.to(device, non_blocking=True)
        text_cond_tuple = collate_text_cond_tuple(kv_lens, device)

        # infer scale schedule via VAE latent
        h, _, _ = vae.encode_for_raw_features(img_B3HW, scale_schedule=None)
        if h.ndim == 5:
            _, _, Tt, HH, WW = h.shape
        else:
            Tt = 1
            _, _, HH, WW = h.shape

        schedule = get_latent2scale_schedule(Tt, int(HH), int(WW), mode=vae.quantizer.schedule_mode)
        _, _, all_bit_indices, _, _, _ = vae.quantizer(h, scale_schedule=schedule)

        bitidx_per_scale: List[torch.Tensor] = []
        for bits in all_bit_indices:
            if bits.ndim == 5:
                bits = bits[:, 0:1]
            elif bits.ndim == 4:
                bits = bits.unsqueeze(1)
            bitidx_per_scale.append(bits.to(torch.uint8).to(device))

        scale_schedule = [(1, int(b.shape[2]), int(b.shape[3])) for b in bitidx_per_scale]
        K = len(scale_schedule)
        k_transmit_eff = min(k_transmit, K)

        # build x tokens for gate
        bsc_args = argparse.Namespace(
            noise_apply_layers=0, noise_apply_requant=0, noise_apply_strength=0.0,
            apply_spatial_patchify=int(args.apply_spatial_patchify), debug_bsc=0,
        )
        bsc = BitwiseSelfCorrection(vae, bsc_args)

        if args.apply_spatial_patchify:
            vae_scale_schedule = [(pt, 2 * ph, 2 * pw) for (pt, ph, pw) in scale_schedule]
        else:
            vae_scale_schedule = scale_schedule

        raw_features, _, _ = vae.encode_for_raw_features(img_B3HW, scale_schedule=vae_scale_schedule)
        x_BLC_wo_prefix, gt_ms_idx_Bl = bsc.flip_requant(vae_scale_schedule, img_B3HW, raw_features, device)

        # determine bit dim
        d_eff = int(gt_ms_idx_Bl[0].shape[-1])

        # helper: get logits for one scale (deterministic)
        def get_logits_for_scale(prev_bits: List[torch.Tensor], si: int) -> torch.Tensor:
            gt_list = list(prev_bits)
            # pad up to si
            while len(gt_list) <= si:
                _, Hs, Ws = scale_schedule[len(gt_list)]
                dummy = torch.zeros((1, 1, Hs, Ws, d_eff), device=device, dtype=torch.uint8)
                gt_list.append(dummy)

            out = gpt.autoregressive_infer_cfg(
                vae=vae,
                scale_schedule=scale_schedule,
                label_B_or_BLT=text_cond_tuple,
                B=1,
                g_seed=None,
                cfg_sc=3,
                cfg_list=[1.0] * len(scale_schedule),
                tau_list=[1.0] * len(scale_schedule),
                top_k=0,
                top_p=0.0,
                returns_vemb=1,
                gumbel=0,
                ret_img=False,
                trunk_scale=int(si + 1),
                gt_leak=int(si + 1),
                gt_ls_Bl=gt_list,
                inference_mode=True,
                sampling_per_bits=1,
                vae_type=1,
                return_logits_per_scale=True,
                do_sample=False,
            )
            return out[-1][si]  # logits for scale si: [1, Ls, 2*d_eff]

        reconstructed_bits: List[torch.Tensor] = []
        keep_masks: List[torch.Tensor] = []

        for si in range(k_transmit_eff):
            bits_gt = bitidx_per_scale[si]  # [1,1,H,W,d]
            _, _, Hs, Ws, _ = bits_gt.shape
            Ls = Hs * Ws

            logits = get_logits_for_scale(reconstructed_bits, si)  # [1,Ls,2d]
            p1 = p1_from_logits(logits)[:, :, :d_eff]
            pred = (p1 >= 0.5).to(torch.uint8)  # [1,Ls,d_eff]

            if si == 0:
                keep_mask = torch.ones((1, int(Hs), int(Ws)), device=device, dtype=torch.bool)
            else:
                feat = segment_x_tokens_to_scale(x_BLC_wo_prefix, scale_schedule, si)
                scores = gate(feat).squeeze(1)  # [1,H,W]
                keep_mask = topk_mask_from_scores(scores, keep_ratio=keep_ratio)

            gt_flat = bits_gt[:, 0].reshape(1, Ls, -1)[:, :, :d_eff]
            rec = pred.clone()
            kept_pos = keep_mask.view(1, -1).nonzero(as_tuple=False)[:, 1]
            rec.index_copy_(1, kept_pos, gt_flat.index_select(1, kept_pos))

            rec_full = torch.zeros((1, 1, int(Hs), int(Ws), d_eff), device=device, dtype=torch.uint8)
            rec_full[:, 0].reshape(1, Ls, d_eff)[:] = rec
            reconstructed_bits.append(rec_full)
            keep_masks.append(keep_mask)

        forced_bitidx_per_scale: List[Optional[torch.Tensor]] = [None] * K
        forced_mask_per_scale: List[Optional[torch.Tensor]] = [None] * K
        for si in range(k_transmit_eff):
            forced_bitidx_per_scale[si] = reconstructed_bits[si]
            forced_mask_per_scale[si] = keep_masks[si]

        out = gpt.autoregressive_infer_cfg(
            vae=vae,
            scale_schedule=scale_schedule,
            label_B_or_BLT=text_cond_tuple,
            B=1,
            g_seed=None,
            cfg_sc=3,
            cfg_list=[1.0] * len(scale_schedule),
            tau_list=[1.0] * len(scale_schedule),
            top_k=0,
            top_p=0.0,
            returns_vemb=1,
            gumbel=0,
            ret_img=True,
            trunk_scale=int(K),
            inference_mode=True,
            sampling_per_bits=1,
            vae_type=1,
            forced_bitidx_per_scale=forced_bitidx_per_scale,
            forced_mask_per_scale=forced_mask_per_scale,
            return_logits_per_scale=False,
            do_sample=False,
        )

        img_u8 = out[2]
        if isinstance(img_u8, list):
            img_u8 = img_u8[-1]
        # expected: [B,H,W,3] uint8
        pred = img_u8.permute(0, 3, 1, 2).float().div(255.0)
        gt = (img_B3HW * 0.5 + 0.5).clamp(0, 1)

        v_lp = float(lpips(pred, gt).mean().item())
        v_di = float(dists(pred, gt).mean().item())
        sum_lp += v_lp
        sum_di += v_di
        tot += 1

    t = torch.tensor([sum_lp, sum_di, float(tot)], device=device, dtype=torch.float32)
    if dist.is_initialized():
        dist.all_reduce(t, op=dist.ReduceOp.SUM)
    sum_lp, sum_di, tot = t.tolist()
    tot = max(1, int(tot))
    mean_lp = sum_lp / tot
    mean_di = sum_di / tot
    score = mean_lp + mean_di
    return score, {"lpips": mean_lp, "dists": mean_di, "score": score}


# =========================================================
# Main
# =========================================================
def main():
    args = parse_args()
    ddp_setup(args.seed)
    device = torch.device("cuda", torch.cuda.current_device())

    if is_main():
        print("[ARGS]", vars(args))

    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # data
    res_list = [int(x) for x in str(args.fixed_hw).split(",")]
    ds_train = FlickrJsonOfflineT5(args.train_json, args.emb_dir, res_list=res_list, seed=args.seed + 1)
    ds_val = FlickrJsonOfflineT5(args.val_json, args.emb_dir, res_list=[res_list[0]], seed=args.seed + 2)

    sampler_train = DistributedSampler(ds_train, shuffle=True, drop_last=True) if dist.is_initialized() else None
    sampler_val = DistributedSampler(ds_val, shuffle=False, drop_last=False) if dist.is_initialized() else None

    def collate_fn(batch):
        imgs = torch.stack([b["img"] for b in batch], dim=0)
        kv_lens = [(b["kv"], b["len"]) for b in batch]
        return imgs, kv_lens

    ld_train = DataLoader(
        ds_train,
        batch_size=args.batch_size,
        sampler=sampler_train,
        shuffle=(sampler_train is None),
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=True,
        collate_fn=collate_fn,
    )
    ld_val = DataLoader(
        ds_val,
        batch_size=1,
        sampler=sampler_val,
        shuffle=False,
        num_workers=max(1, args.num_workers // 2),
        pin_memory=True,
        drop_last=False,
        collate_fn=collate_fn,
    )

    # build models
    vae = build_vae(args, device)
    gpt = build_gpt(args, vae, device)

    # inject LoRA BEFORE FSDP
    gpt = inject_lora(
        gpt,
        r=int(args.lora_r),
        alpha=float(args.lora_alpha),
        dropout=float(args.lora_dropout),
        target_regex=str(args.lora_target_regex),
        exclude_regex=str(args.lora_exclude_regex),
        verbose=is_main(),
    )
    # freeze everything except trainables (LoRA)
    for p in gpt.parameters():
        if not p.requires_grad:
            p.requires_grad = False

    # infer Gate input channels conservatively
    # In your BitwiseSelfCorrection, x tokens are reshaped from VAE features:
    # - no patchify: channels = vae.embed_dim
    # - patchify: channels = 4*vae.embed_dim
    in_ch = int(getattr(vae, "embed_dim", 256)) * (4 if args.apply_spatial_patchify else 1)
    gate = GateNet(in_ch=in_ch, hidden=int(args.gate_hidden), depth=int(args.gate_depth)).to(device)

    # wrap with FSDP if requested
    if args.fsdp != 0:
        gpt = wrap_fsdp_if_needed(gpt, args)
        gate = FSDP(
            gate,
            device_id=torch.cuda.current_device(),
            sharding_strategy=(ShardingStrategy.FULL_SHARD if args.fsdp == 3 else ShardingStrategy.SHARD_GRAD_OP),
            use_orig_params=True,
            sync_module_states=True,
            limit_all_gathers=True,
        )

    # optimizer
    lora_params = [p for p in gpt.parameters() if p.requires_grad]
    gate_params = [p for p in gate.parameters() if p.requires_grad]
    if is_main():
        print(f"[PARAMS] trainable: lora={sum(p.numel() for p in lora_params):,} gate={sum(p.numel() for p in gate_params):,}")

    opt = torch.optim.AdamW(
        [{"params": lora_params, "lr": float(args.lr), "weight_decay": float(args.weight_decay)},
         {"params": gate_params, "lr": float(args.gate_lr), "weight_decay": float(args.weight_decay)}],
        betas=(0.9, 0.999),
    )

    # bitwise helper
    bsc_args = argparse.Namespace(
        noise_apply_layers=0, noise_apply_requant=0, noise_apply_strength=0.0,
        apply_spatial_patchify=int(args.apply_spatial_patchify), debug_bsc=0,
    )
    bsc = BitwiseSelfCorrection(vae, bsc_args)

    # random budget RNG per-rank
    rank = dist.get_rank() if dist.is_initialized() else 0
    budget_rng = random.Random(int(args.seed) + 12345 + rank)

    # autocast
    use_autocast = bool(args.bf16) and device.type == "cuda"
    ac_dtype = torch.bfloat16 if args.bf16 else torch.float16

    best_score = float("inf")
    global_step = 0

    for ep in range(int(args.epochs)):
        if sampler_train is not None:
            sampler_train.set_epoch(ep)

        gpt.train()
        gate.train()

        for img_B3HW, kv_lens in ld_train:
            global_step += 1
            img_B3HW = img_B3HW.to(device, non_blocking=True)
            text_cond_tuple = collate_text_cond_tuple(kv_lens, device)

            # determine scale schedule from current latent shape
            with torch.no_grad():
                h, _, _ = vae.encode_for_raw_features(img_B3HW, scale_schedule=None)
                if h.ndim == 5:
                    _, _, Tt, HH, WW = h.shape
                else:
                    Tt = 1
                    _, _, HH, WW = h.shape

                schedule = get_latent2scale_schedule(Tt, int(HH), int(WW), mode=vae.quantizer.schedule_mode)
                _, _, all_bit_indices, _, _, _ = vae.quantizer(h, scale_schedule=schedule)

                bitidx_per_scale: List[torch.Tensor] = []
                for bits in all_bit_indices:
                    if bits.ndim == 5:
                        bits = bits[:, 0:1]
                    elif bits.ndim == 4:
                        bits = bits.unsqueeze(1)
                    bitidx_per_scale.append(bits.to(torch.uint8))

                scale_schedule = [(1, int(b.shape[2]), int(b.shape[3])) for b in bitidx_per_scale]

                if args.apply_spatial_patchify:
                    vae_scale_schedule = [(pt, 2 * ph, 2 * pw) for (pt, ph, pw) in scale_schedule]
                else:
                    vae_scale_schedule = scale_schedule

                raw_features, _, _ = vae.encode_for_raw_features(img_B3HW, scale_schedule=vae_scale_schedule)

            x_BLC_wo_prefix, gt_ms_idx_Bl = bsc.flip_requant(vae_scale_schedule, img_B3HW, raw_features, device)

            # training scales truncate
            training_scales = min(int(args.always_training_scales), len(scale_schedule))
            scale_schedule_tr = scale_schedule[:training_scales]
            lens_each = [int(np.prod(s)) for s in scale_schedule_tr]
            training_seq_len = int(sum(lens_each))
            first_len = int(lens_each[0])

            # x excludes scale0 tokens => length sum(scale1..)
            x_BLC_wo_prefix = x_BLC_wo_prefix[:, :(training_seq_len - first_len), :]

            # gt bits: concat across scales (includes scale0)
            gt_BLd = torch.cat(gt_ms_idx_Bl, dim=1)[:, :training_seq_len, :].contiguous().long()
            d_eff = int(gt_BLd.shape[-1])

            # random budgets
            kmin = max(2, int(args.k_transmit_min))
            kmax = max(kmin, int(args.k_transmit_max))
            k_transmit = budget_rng.randint(kmin, min(kmax, training_scales))

            kr_min = float(args.keep_ratio_min)
            kr_max = float(args.keep_ratio_max)
            keep_ratios = [budget_rng.random() * (kr_max - kr_min) + kr_min for _ in range(training_scales)]

            opt.zero_grad(set_to_none=True)

            with torch.autocast("cuda", dtype=ac_dtype, enabled=use_autocast):
                logits_BLV = gpt(label_B_or_BLT=text_cond_tuple, x_BLC_wo_prefix=x_BLC_wo_prefix, scale_schedule=scale_schedule_tr)
                loss_bits = bits_loss_from_logits(logits_BLV, gt_BLd)

                gate_losses: List[torch.Tensor] = []
                # supervise scales 1..k_transmit-1
                for si in range(1, k_transmit):
                    kr = float(keep_ratios[si])
                    with torch.no_grad():
                        tmask = teacher_entropy_spatial_mask(logits_BLV, scale_schedule_tr, si, d_eff=d_eff, keep_ratio=kr)  # [B,H,W]
                    feat = segment_x_tokens_to_scale(x_BLC_wo_prefix, scale_schedule_tr, si)
                    pred_logits = gate(feat).squeeze(1)  # [B,H,W]
                    gate_losses.append(F.binary_cross_entropy_with_logits(pred_logits, tmask.float(), reduction="mean"))

                loss_gate = torch.stack(gate_losses).mean() if gate_losses else torch.zeros((), device=device)
                loss = loss_bits + float(args.gate_loss_weight) * loss_gate

            loss.backward()
            if float(args.grad_clip) > 0:
                torch.nn.utils.clip_grad_norm_(lora_params + gate_params, max_norm=float(args.grad_clip))
            opt.step()

            if is_main() and (global_step % 50 == 0):
                print(f"[train] ep={ep} step={global_step} loss={loss.item():.4f} bits={loss_bits.item():.4f} gate={loss_gate.item():.4f} kT={k_transmit}")

            # validation + best save
            if int(args.val_every) > 0 and (global_step % int(args.val_every) == 0):
                score, mets = validate(args, vae, gpt, gate, ld_val, device)
                if is_main():
                    print(f"[val] step={global_step} score={score:.6f} lpips={mets['lpips']:.6f} dists={mets['dists']:.6f}")

                if score < best_score and is_main():
                    best_score = score
                    ckpt = {
                        "step": global_step,
                        "best_score": best_score,
                        "metrics": mets,
                        "gpt_trainable": trainable_state_dict_fsdp(gpt),
                        "gate_trainable": trainable_state_dict_fsdp(gate),
                        "args": vars(args),
                    }
                    ckpt_path = out_dir / "best_trainable.pt"
                    torch.save(ckpt, ckpt_path)
                    print(f"[SAVE] best -> {ckpt_path}")

    if is_main():
        print(f"[DONE] best_score={best_score:.6f} (see {out_dir/'best_trainable.pt'})")


if __name__ == "__main__":
    main()
